{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9eb459b",
   "metadata": {
    "id": "e9eb459b"
   },
   "source": [
    "<font size=\"5\">**NEURAL NETWORK**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e18db8",
   "metadata": {
    "id": "66e18db8"
   },
   "source": [
    "**PRE-PROCESSING STEPS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6384e32e",
   "metadata": {
    "id": "6384e32e"
   },
   "source": [
    "The following cell has steps involving importing of necessary libraries and also defining functions for pre-processing steps such as removal of emojis, punctuations from the reviews. Also, function for removal of stopwords along with the function for lemmatization.\n",
    "Next step involves reading the csv and storing it in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ceeb57f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7ceeb57f",
    "outputId": "841a3916-eacc-489e-b4b9-94238c572db3"
   },
   "outputs": [],
   "source": [
    "#Importing necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "\n",
    "import emoji\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#Pre-processing functions\n",
    "\n",
    "def remove_emoji(string):\n",
    "    return emoji.replace_emoji(string, '')\n",
    "\n",
    "\n",
    "def stop_word_list():\n",
    "    nltk.download('stopwords')\n",
    "    #Getting stopwords\n",
    "    sw = stopwords.words('english')\n",
    "\n",
    "    #Removing the important stopwords from the corpus\n",
    "    sw.remove('not')\n",
    "    sw.remove(\"didn't\")\n",
    "    sw.remove(\"don't\")\n",
    "    sw.remove(\"wasn't\")\n",
    "\n",
    "    return sw\n",
    "\n",
    "def pos_tagger(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:         \n",
    "        return None\n",
    "\n",
    "def lemmatize(text):\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in text:\n",
    "        if tag is None:\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:       \n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    lemmatized_sentence = \" \".join(lemmatized_sentence)\n",
    "    return lemmatized_sentence\n",
    "\n",
    "#Reading CSV\n",
    "df = pd.read_csv(r'Labelled_Dataset.csv')\n",
    "\n",
    "df.columns = ['LABEL','COMMENTS']\n",
    "\n",
    "#Removing unnecessary spaces from the labels\n",
    "labels = df['LABEL'].tolist()\n",
    "labels = [label.strip() for label in labels]\n",
    "df['LABEL'] = labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0cbce2",
   "metadata": {
    "id": "8f0cbce2"
   },
   "source": [
    "**SPLITTING DATA**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7f28bb",
   "metadata": {
    "id": "7d7f28bb"
   },
   "source": [
    "The next cell has a function of splitting the data into train and test set. The size of training set is 80% of the total dataset whereas the rest of the 20% of the whole dataset makes up the test set. The random state parameter is used and set to a constant integer, in this case 42, to make the splitting of data deterministic and reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3eb601a",
   "metadata": {
    "id": "b3eb601a"
   },
   "outputs": [],
   "source": [
    "# Function for splitting of data \n",
    "\n",
    "def Split_data(x,y):\n",
    "    \n",
    "    #Importing the necessary library\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    df_train,df_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42)\n",
    "    return(df_train,df_test,y_train,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e7f828",
   "metadata": {
    "id": "85e7f828"
   },
   "source": [
    "**CONFUSION MATRIX**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbf7616",
   "metadata": {
    "id": "6dbf7616"
   },
   "source": [
    "The following function will create the confusion matrix and will also give precision, recall and f1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ac9060",
   "metadata": {
    "id": "e5ac9060"
   },
   "outputs": [],
   "source": [
    "#Confusion Matrix\n",
    "\n",
    "def Create_Confusion_Matrix(model, df_test, y_test):\n",
    "    \n",
    "    #Importing necessary libraries\n",
    "    \n",
    "    from sklearn import metrics\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import precision_score\n",
    "    from sklearn.metrics import recall_score\n",
    "    from sklearn.metrics import f1_score\n",
    "    \n",
    "    #Getting predicted classes and actual classes\n",
    "    \n",
    "    predicted_classes = model.predict(df_test)\n",
    "    classes_x=np.argmax(predicted_classes,axis=1)\n",
    "    y_true = y_test\n",
    "\n",
    "    #Printing the precision,recall and F1 metrics \n",
    "    \n",
    "    print(metrics.classification_report(y_true, classes_x, digits=3))\n",
    "    \n",
    "    #Constructing the confusion matrix\n",
    "    \n",
    "    labels = ['Negative', 'Neutral', 'Positive']\n",
    "    cm = confusion_matrix(y_true, classes_x)\n",
    "    plt.figure(figsize = (5, 5))\n",
    "    ax= plt.subplot()\n",
    "    sns.heatmap(cm, annot=True, fmt='g', ax=ax);\n",
    "    ax.xaxis.set_ticklabels(labels);\n",
    "    ax.yaxis.set_ticklabels(labels);\n",
    "    ax.set_xlabel('Predicted labels');\n",
    "    ax.set_ylabel('True labels'); \n",
    "    ax.set_title('Confusion Matrix'); \n",
    "    print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68befcac",
   "metadata": {},
   "source": [
    "**MODEL**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1fa51f",
   "metadata": {},
   "source": [
    "The Neural Network used consists of a input layer, one hidden layer and a output layer. Input and hidden layer both have 120 units. The activation function used in input and hidden layer is ReLu and Softmax activation function is used in the output layer. 40-60 epochs were yielding similar results and as we increased the epochs beyond 60 the test accuracy was decreasing because of overfitting.\n",
    "Seeding was used to make the model reproducible as without seeding the model was getting trained differently with each run. The count vectorizer was used as the tf-idf vectorizer was yielding less accuracy. The reason for count vectorizer doing better in our case is data length is short and with less unique words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cde75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for creating a model\n",
    "\n",
    "def Neural_Network_Model(df_train,df_test,y_train,y_test,x,y):\n",
    "    \n",
    "    #Importing necessary libraries\n",
    "    \n",
    "    import tensorflow as tf\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    import keras\n",
    "    from numpy.random import seed\n",
    "    import random as rn\n",
    "    import os\n",
    "    \n",
    "    #Seeding\n",
    "    os.environ['PYTHONHASHSEED'] = '0'\n",
    "    np.random.seed(37)\n",
    "    rn.seed(1254)\n",
    "    tf.random.set_seed(89)\n",
    "\n",
    "    vectorizer = CountVectorizer()\n",
    "\n",
    "    vectorizer.fit(x)\n",
    "    df_train = vectorizer.transform(df_train)\n",
    "    df_test = vectorizer.transform(df_test)\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    le.fit(y)\n",
    "    y_train = le.transform(y_train)\n",
    "    y_test = le.transform(y_test)\n",
    "\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    #Input layer\n",
    "    \n",
    "    model.add(keras.layers.Dense(units=120, activation='relu', input_dim=len(vectorizer.get_feature_names_out())))\n",
    "    \n",
    "    #Hidden layer\n",
    "    \n",
    "    model.add(keras.layers.Dense(units=120, activation='relu', input_dim=120))\n",
    "    \n",
    "    #Output layer\n",
    "    \n",
    "    model.add(keras.layers.Dense(units=3, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    #Training the model\n",
    "    history = model.fit(df_train, y_train, \n",
    "              epochs=40, verbose=0)\n",
    "    scores = model.evaluate(df_test, y_test, verbose=1)\n",
    "    print(\"Accuracy:\", scores[1])\n",
    "    Create_Confusion_Matrix(model, df_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e23cf28",
   "metadata": {},
   "source": [
    "**FIRST CASE: EMOJI REMOVAL**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3461ae0",
   "metadata": {
    "id": "d3461ae0"
   },
   "source": [
    "In this case, we just removed the emojis from the comment, split the processed dataset and trained the neural network. After training the model with emojiless data the test accuracy came out to be around 83.33%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e630b75",
   "metadata": {
    "id": "9e630b75"
   },
   "outputs": [],
   "source": [
    "#Removing emoji\n",
    "\n",
    "for items in range(0,df.shape[0]):\n",
    "    df.at[items, 'CLEAN COMMENTS']= remove_emoji(df.at[items, 'COMMENTS'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e829bf6",
   "metadata": {
    "id": "7e829bf6"
   },
   "source": [
    "**SIXTH CASE: STOP WORD REMOVAL WITH LEMMATISATION**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2a6bfb",
   "metadata": {
    "id": "8a2a6bfb"
   },
   "source": [
    "In this case, we applied lemmatisation on the data used for the above(fifth) case, split the processed dataset and trained the neural network. After training the model with this data the test accuracy came out to be around 84.58%.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dace2dcf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dace2dcf",
    "outputId": "5a78bcc9-faed-42c3-9e37-763eaaee9f57"
   },
   "outputs": [],
   "source": [
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "#Creating Tokens\n",
    "df['TOKEN'] = df['CLEAN COMMENTS'].apply(word_tokenize)\n",
    "\n",
    "#PoS Tagging\n",
    "nltk.download('omw-1.4')\n",
    "df['POS TAGGING'] = df['TOKEN'].apply(nltk.pos_tag)\n",
    "for items in range(0,df.shape[0]):\n",
    "    df.at[items, 'POS TAGGING'] = list(map(lambda x: (x[0], pos_tagger(x[1])),df.at[items, 'POS TAGGING']))\n",
    "\n",
    "#Lemmatization\n",
    "for items in range(0,df.shape[0]):\n",
    "    df.at[items,'LEMM'] = lemmatize(df.at[items,'POS TAGGING'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977d769d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 717
    },
    "id": "977d769d",
    "outputId": "37c10891-0f76-4575-a934-7ead44aae4df"
   },
   "outputs": [],
   "source": [
    "df_train,df_test,y_train,y_test = Split_data(df['LEMM'],df['LABEL'])\n",
    "Neural_Network_Model(df_train,df_test,y_train,y_test,x=df['LEMM'],y=df['LABEL'])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
